{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/adasegroup/ML2020_seminars/blob/master/seminar5/Trees_Bagging_Random%20Forest_solutions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seminar: Trees, Bootstrap Aggregation (Bagging) and Random Forest\n",
    "Machine Learning by professor Evgeny Burnaev\n",
    "<br\\>\n",
    "Author: Andrey Lange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "pd.set_option('display.max_rows', 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 1: Regression tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare and show a dataset\n",
    "n = 1                         # number of features\n",
    "N = 100**n                    # number of samples\n",
    "np.random.seed(0)\n",
    "X = np.random.random((N, n))*3\n",
    "coeffs = 1 +  2 * np.random.random((n, 1))\n",
    "y = np.sin(np.matmul(X*X, coeffs)) + np.random.random((N, 1))/3\n",
    "y = y.ravel()\n",
    "print((pd.DataFrame({'x1': X[:, 0], 'y': y})))\n",
    "plt.plot(X, y, '*')\n",
    "plt.title('1 predictor: x1; target: y')\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('y')\n",
    "\n",
    "# train and predict a regression tree\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "clf = DecisionTreeRegressor(max_depth=1)\n",
    "clf.fit(X, y)\n",
    "y_pred = clf.predict(X)\n",
    "\n",
    "plt.plot(X, y_pred, '.r')\n",
    "plt.show()\n",
    "\n",
    "print('Mean Squared Error: ', mean_squared_error(y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.1: Change the number of levels in a regression tree above until the best approximation of the training set. What is the best MSE?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 1.1:\n",
    "When `max_depth=12` then \n",
    "<br>\n",
    "`Mean Squared Error:  0.0`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.2: Calculate MSE above without `mean_squared_error()` calling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 1.2:\n",
    "\n",
    "```\n",
    "print('Mean Squared Error: ', ((y - y_pred)**2).mean())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Regression tree with $n=2$ features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.1: Lets try an example with $n=2$ features. Train a regression stump (a tree of a depth 1) and see the optimal threshold (border between colors) for the best feature chosen for split among $x_1$ and $x_2$. What feature was chosen and why? Change something in the string \n",
    "```\n",
    "coeffs = np.array([[0.2], [1.5]])\n",
    "```\n",
    "### to make another feature is chosen as the best for split.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.2: Find the optimal `max_depth` hyperparameter when MSE on the training set is the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare and show a dataset in 2D\n",
    "n = 2               # number of features\n",
    "N = 100**n          # number of samples\n",
    "np.random.seed(0)\n",
    "X = np.random.random((N, n))*1.8\n",
    "\n",
    "# Change something to make another feature is chosen for split by some optimal threshold\n",
    "coeffs = np.array([[0.2], [1.5]])\n",
    "y = np.sin(np.matmul(X*X, coeffs)) + np.random.random((N, 1))/3\n",
    "\n",
    "# print dataset\n",
    "print((pd.DataFrame({'x1': X[:, 0], 'x2': X[:, 1], 'y': y.ravel()})))\n",
    "\n",
    "# show target y in (x1, x2) space\n",
    "plt.figure(figsize=[9, 6])\n",
    "sc = plt.scatter(X[:, 0], X[:, 1], c=y.ravel(), s=5)\n",
    "plt.colorbar(sc)\n",
    "plt.title('training data: predictors x1, x2 and target y')\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "plt.show()\n",
    "\n",
    "# train and predict by a regression tree\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "clf = DecisionTreeRegressor(max_depth=1)\n",
    "clf.fit(X, y)\n",
    "y_pred = clf.predict(X)\n",
    "\n",
    "# show prediction in (x1, x2) space\n",
    "plt.figure(figsize=[9, 6])\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_pred, s=5)\n",
    "plt.colorbar(sc)\n",
    "plt.title('prediction on the training data')\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "plt.show()\n",
    "\n",
    "print('Mean Squared Error: ', mean_squared_error(y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 2.1:\n",
    "\n",
    "When the range of any feature is split by optimal threshold each of 2 regions are approximated by constant values. The feature which provides better approximation of the function for each region is chosen.\n",
    "\n",
    "You can simply swap the coefficients.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: Regression tree: training and testing sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.1: Now we consider training and testing sets. Try different depths of a decision tree to see when the model is underfitted and when the one is overfitted to the training set. Plot the MSE on the testing set depending on `max_depth` hyperparameter. What is the optimal value?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# prepare dataset\n",
    "n = 1                         # number of features\n",
    "N = 200**n                    # number of samples\n",
    "np.random.seed(0)\n",
    "X = np.random.random((N, n))*3\n",
    "coeffs = 1 +  2 * np.random.random((n, 1))\n",
    "y = np.sin(np.matmul(X*X, coeffs)) + np.random.random((N, 1))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1/2, random_state=0)\n",
    "\n",
    "# --- change this block to select the best max_depth\n",
    "clf = DecisionTreeRegressor(max_depth=1)\n",
    "clf.fit(X_train, y_train)\n",
    "print('train MSE: ', mean_squared_error(y_train, clf.predict(X_train)))\n",
    "print('test MSE: ', mean_squared_error(y_test, clf.predict(X_test)))\n",
    "# ---\n",
    "\n",
    "plt.figure(figsize=[9, 6])\n",
    "plt.plot(X_train, y_train, '*')\n",
    "plt.plot(X_train, clf.predict(X_train), '.r')\n",
    "plt.title('training dataset')\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('y')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=[9, 6])\n",
    "plt.plot(X_test, y_test, '*')\n",
    "plt.plot(X_test, clf.predict(X_test), '.r')\n",
    "plt.title('testing dataset')\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 3.1: \n",
    "\n",
    "Add this code to show the optimal testing MSE:\n",
    "\n",
    "```\n",
    "test_mse = []\n",
    "max_depth_range = range(1, 15)\n",
    "for md in max_depth_range:\n",
    "    clf = DecisionTreeRegressor(max_depth=md)\n",
    "    clf.fit(X_train, y_train)\n",
    "    test_mse += [mean_squared_error(y_test, clf.predict(X_test))]\n",
    "plt.plot(max_depth_range, test_mse)\n",
    "plt.title('testing MSE')\n",
    "plt.xlabel('max_depth')\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 4: Bagging = Decision Tree + Bootstrap\n",
    "### In the question above we've found the optimal `max_depth` for the case of single Decision Tree. By limiting the tree depth we distort the fitting to the training dataset and prevent the model from overfitting. \n",
    "### The second way to prevent overfitting is to distort the training dataset...What is Bagging?\n",
    "### We train many trees each on a Bootstraped training dataset (it contains the same number of samples but some of them are included with some number of their copies, and some of them are not included). Then we average over all such trees. It is called Bootstrap aggregation - Bagging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4.1: Compare the prediction above (single tree) with Bagging all with `max_depth=5`. Why Bagging approximation red dots does not look like constant-values regions as in tree? Tune the best number of trees. Has  Bagging improved the single tree model regarding `test MSE`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4.2: Sometimes it is good to be a perfectionist and unittest any code :). Unittest scikit-learn! Should `DecisionTreeRegressor` and `BaggingRegressor`  give the same results in some special case? Simplify Bagging to the single Decision Tree and show the same pictures as above when `max_depth=5`. What parameter `n_estimators` have to be set to? Do we need to change any other hyperparameters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# prepare dataset\n",
    "n = 1                         # number of features\n",
    "N = 200**n                    # number of samples\n",
    "np.random.seed(0)\n",
    "X = np.random.random((N, n))*3\n",
    "coeffs = 1 +  2 * np.random.random((n, 1))\n",
    "y = np.sin(np.matmul(X*X, coeffs)) + np.random.random((N, 1))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1/2, random_state=0)\n",
    "\n",
    "# --- 1. change this block to select the best n_estimators\n",
    "# --- 2. change this block to simplify Bagging to ordinary single decision tree\n",
    "clf = BaggingRegressor(DecisionTreeRegressor(max_depth=5), n_estimators=10, bootstrap=True, random_state=0)\n",
    "clf.fit(X_train, y_train.ravel())\n",
    "print('train MSE: ', mean_squared_error(y_train, clf.predict(X_train)))\n",
    "print('test MSE: ', mean_squared_error(y_test, clf.predict(X_test)))\n",
    "# ---\n",
    "\n",
    "plt.figure(figsize=[9, 6])\n",
    "plt.plot(X_train, y_train, '*')\n",
    "plt.plot(X_train, clf.predict(X_train), '.r')\n",
    "plt.title('training dataset')\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('y')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=[9, 6])\n",
    "plt.plot(X_test, y_test, '*')\n",
    "plt.plot(X_test, clf.predict(X_test), '.r')\n",
    "plt.title('testing dataset')\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Solution 4.1:\n",
    "\n",
    "We average over different trees each a little differs from another because they are trained on randomly bootstraped datasets. It results in a more smoothed and better picture of red dots.\n",
    "\n",
    "Use this code to tune `n_estimators`:\n",
    "```\n",
    "test_mse = []\n",
    "n_estimators_range = range(1, 50)\n",
    "for ne in n_estimators_range:\n",
    "    clf = BaggingRegressor(DecisionTreeRegressor(max_depth=5), n_estimators=ne, bootstrap=True, random_state=0)\n",
    "    clf.fit(X_train, y_train.ravel())\n",
    "    test_mse += [mean_squared_error(y_test, clf.predict(X_test))]\n",
    "plt.plot(n_estimators_range, test_mse)\n",
    "plt.title('testing MSE')\n",
    "plt.xlabel('n_estimators')\n",
    "plt.show()\n",
    "```\n",
    "Bagging reduces test MSE - improvement!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 4.2: \n",
    "\n",
    "```n_estimators=1, bootstrap=False```\n",
    "\n",
    "is the equivalent to single Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 5: Random Forest = Bagging + (`max_features' < $n$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Can we reduce the overfitting more? - YES!!! <br> Along with Bootstrap reduce the number of features among which the best feature for each tree in ensemble is chosen!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5.1: Tune `max_features` to improve the tesing MSE. Check that testing MSE becomes better. And what happens with training MSE and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# prepare dataset\n",
    "n = 100           # number of features\n",
    "N = 10**4         # number of samples\n",
    "np.random.seed(0)\n",
    "X = np.random.random((N, n))*3\n",
    "coeffs = 1 +  2 * np.random.random((n, 1))\n",
    "y = np.sin(np.matmul(X*X, coeffs)) + np.random.random((N, 1))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1/2, random_state=0)\n",
    "\n",
    "# --- change this block to select the best max_features\n",
    "clf = RandomForestRegressor(max_depth=5, n_estimators=10, max_features=10, random_state=0)\n",
    "clf.fit(X_train, y_train.ravel())\n",
    "print('train MSE: ', mean_squared_error(y_train, clf.predict(X_train)))\n",
    "print('test MSE: ', mean_squared_error(y_test, clf.predict(X_test)))\n",
    "# ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 5.1: \n",
    "\n",
    "One must reduce `max_features`, in this particular case down to 1.\n",
    "<br>\n",
    "Testing MSE becomes better (decreases) although training MSE becomes worse (increases) and this is OK!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 6: Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from Kaggle\n",
    "\n",
    "X_train = pd.read_csv('https://raw.githubusercontent.com/adasegroup/ML2020_seminars/master/seminar5/data/give_me_some_credit.csv', index_col=0)\n",
    "X_train = X_train.dropna()\n",
    "y_train = X_train['SeriousDlqin2yrs']\n",
    "X_train = X_train.drop(['SeriousDlqin2yrs'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "n_features = X_train.shape[1]\n",
    "clf = GridSearchCV(RandomForestClassifier(),\n",
    "                   # you can play with tuning, up to your CPU performance:\n",
    "#                  {'max_depth': [1, 3, 5], 'n_estimators': [10, 50, 150], 'max_features': ['auto', n_features]},\n",
    "                   {'max_depth': [5], 'n_estimators': [50], 'max_features': [n_features]},\n",
    "                   cv = 3)\n",
    "clf.fit(X_train, y_train)\n",
    "print('best accuracy score:', clf.best_score_) # validation score\n",
    "\n",
    "# now let's draw ROC curve\n",
    "plt.figure(figsize=[9, 6])\n",
    "fpr, tpr, _ = roc_curve(y_train, clf.predict_proba(X_train)[:, 1])\n",
    "plt.plot(fpr, tpr, 'r', label='train')\n",
    "plt.title('ROC curves')\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.show()\n",
    "\n",
    "clf.best_params_\n",
    "\n",
    "# feature importances\n",
    "fi = pd.Series(clf.best_estimator_.feature_importances_, index=X_train.columns)\n",
    "fi.sort_values(ascending=False).plot(kind='bar')\n",
    "plt.title('feature importances')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Be careful with feature_importances: Try to add copies of the most important feature and the new importances are not the same:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('https://raw.githubusercontent.com/adasegroup/ML2020_seminars/master/seminar5/data/give_me_some_credit.csv', index_col=0)\n",
    "X_train = X_train.dropna()\n",
    "y_train = X_train['SeriousDlqin2yrs']\n",
    "X_train = X_train.drop(['SeriousDlqin2yrs'], axis=1)\n",
    "\n",
    "X_train['NumberOfTimes90DaysLate_2'] = X_train['NumberOfTimes90DaysLate']\n",
    "X_train['NumberOfTimes90DaysLate_3'] = X_train['NumberOfTimes90DaysLate']\n",
    "\n",
    "n_features = X_train.shape[1]\n",
    "clf = GridSearchCV(RandomForestClassifier(),\n",
    "                   # you can play with tuning, up to your CPU performance:\n",
    "                   {'max_depth': [5], 'n_estimators': [50], 'max_features': [n_features]},\n",
    "                   cv = 3)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# feature importances\n",
    "fi = pd.Series(clf.best_estimator_.feature_importances_, index=X_train.columns)\n",
    "fi.sort_values(ascending=False).plot(kind='bar')\n",
    "plt.title('feature importances')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets try to exclude some highly correlated feature, but the importance of others remaining has not increased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[['NumberOfTime60-89DaysPastDueNotWorse', 'NumberOfTime30-59DaysPastDueNotWorse', 'NumberOfTimes90DaysLate']].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('https://raw.githubusercontent.com/adasegroup/ML2020_seminars/master/seminar5/data/give_me_some_credit.csv', index_col=0)\n",
    "X_train = X_train.dropna()\n",
    "y_train = X_train['SeriousDlqin2yrs']\n",
    "X_train = X_train.drop(['SeriousDlqin2yrs'], axis=1)\n",
    "\n",
    "# X_train['NumberOfTimes90DaysLate_2'] = X_train['NumberOfTimes90DaysLate']\n",
    "# X_train['NumberOfTimes90DaysLate_3'] = X_train['NumberOfTimes90DaysLate']\n",
    "X_train = X_train.drop(['NumberOfTime60-89DaysPastDueNotWorse'], axis=1)\n",
    "\n",
    "n_features = X_train.shape[1]\n",
    "clf = GridSearchCV(RandomForestClassifier(),\n",
    "                   # you can play with tuning, up to your CPU performance:\n",
    "                   {'max_depth': [5], 'n_estimators': [50], 'max_features': [n_features]},\n",
    "                   cv = 3)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# feature importances\n",
    "fi = pd.Series(clf.best_estimator_.feature_importances_, index=X_train.columns)\n",
    "fi.sort_values(ascending=False).plot(kind='bar')\n",
    "plt.title('feature importances')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
