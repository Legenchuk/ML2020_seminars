{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/adasegroup/ML2020_seminars/blob/master/seminar9/feature_and_model_selection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A good grasp of feature selection methods leads to better performing models, better understanding of the underlying structure and characteristics of the data and leads to better intuition about the algorithms that underlie many machine learning models.\n",
    "\n",
    "There are in general two reasons why feature selection is used:\n",
    "1. Reducing the number of features, to reduce overfitting and improve the generalization of models.\n",
    "2. To gain a better understanding of the features and their relationship to the response variables.\n",
    "\n",
    "In some cases a model may become more interpretable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Toy function\n",
    "\n",
    "\n",
    "$$\n",
    "y = \\sin (4\\pi x_1 x_2) + 4(x_3 - 0.5)^2 + x_4 + 0.5 x_5 + \\varepsilon,\n",
    "$$\n",
    "where \n",
    "\n",
    "* $x_1, \\ldots, x_5 \\sim Uniform([0, 1])$ and $\\varepsilon \\sim \\mathcal{N}(0, 1)$.\n",
    "* We also add noise variables $x_6, x_7, x_8$\n",
    "* and  $x_9, x_{10}, x_{11}$ strongly correlated with $x_2, x_3, x_4$ generated by $f(x) = x + \\mathcal{N}(0, 025)$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "def toy_function(x, sigma_noise=1, random_state=None):\n",
    "    if random_state is None:\n",
    "        random_state = np.random.RandomState()\n",
    "    y = (np.sin(4 * np.pi * x[:, 0] * x[:, 1])\n",
    "         + 4 * (x[:, 2] - 0.5)**2\n",
    "         + x[:, 3]\n",
    "         + 0.5 * x[:, 4]\n",
    "         + random_state.normal(0, sigma_noise, size=(len(x)))\n",
    "        )\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate training, validation and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "random_state = np.random.RandomState(1)\n",
    "size = 1500\n",
    "X = random_state.rand(size, 11)\n",
    "y = toy_function(X, sigma_noise=0.01, random_state=random_state) \n",
    "\n",
    "#Add 3 additional correlated variables (correlated with X1-X3)\n",
    "X[:, 8:] = X[:, 1:4] + random_state.normal(0, .025, size=(size, 3))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=random_state)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.3, random_state=random_state)\n",
    "print(X_train.shape, X_valid.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F-test based feature importances\n",
    "\n",
    "Consider two models, $M_1$ and $M_2$, where $M_1$ has $p_1$ parameters and $M_2$ has $p_2$ parameters,\n",
    "and $p_2 > p_1$,\n",
    "parameters of $M_1$ are subset of parameters of $M_2$.\n",
    "\n",
    "* We want to check whether $M_2$ significantly better than $M_1$\n",
    "* Let us consider the value\n",
    "    $$\n",
    "    F=\\frac{\\left({\\frac {{\\text{RSS}}_{1}-{\\text{RSS}}_{2}}{p_{2}-p_{1}}}\\right)}\n",
    "           {\\left({\\frac {{\\text{RSS}}_{2}}{n-p_{2}-1}}\\right)},\n",
    "    $$ \n",
    "    where $RSS_i$ is the Residual Sum of Squares of $M_i$.  \n",
    "\n",
    "* If $M_2$ is not significantly better than $M_1$, $F$ will have an F-distribution, with $(p_2−p_1, n−p_2)$ degrees of freedom.\n",
    "* We rejecte the hypothesis if the $F$ value is greater than the critical value of the F-distribution for some desired false-rejection probability (e.g. 0.05).\n",
    "\n",
    "\n",
    "* Relies on $\\mathbf{\\color{orange}{\\mbox{linearity}}}$ of models $M_1, M_2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's apply F-test to the generated data and analyze results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import f_regression\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "feature_names = np.array(\n",
    "    ['x1 [sin(x1 x2)]', 'x2 [sin(x1 x2)]', 'x3 [x3^2]', 'x4', r'$\\frac12$x5',\n",
    "     'noise1', 'noise2', 'noise3',\n",
    "     'corr(x2)', 'corr(x3)', 'corr(x4)'])\n",
    "\n",
    "feature_scores = pd.DataFrame(columns=['F test'], index=feature_names)\n",
    "\n",
    "feature_scores['F test'], _ = f_regression(X_train, y_train)\n",
    "\n",
    "feature_scores['F test'] /= feature_scores['F test'].max()\n",
    "feature_scores.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion**: F-test captures well only linear dependencies!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mutual Information\n",
    "\n",
    "This method estimates how much information about the target variable $Y$ is contained in the feature $X$.  \n",
    "To do this we calculate the value\n",
    "$$\n",
    "MI(X, Y) = H(Y) - H(Y | X) = \\sum_{x, y} p(x, y) \\log \\frac{p(x, y)}{p(x)p(y)} = D_{KL}(p(x, y) \\, ||  \\, p(x)p(y)),\n",
    "$$\n",
    "where $p(x, y), p(x), p(y)$ are joint and marginal densities of random variables $X$ and $Y$.\n",
    "\n",
    "* $MI(X, Y) = 0 \\Leftrightarrow $ $X$ and $Y$ are independent\n",
    "* Higher values $\\rightarrow$ higher dependency\n",
    "* Can capture $\\mathbf{\\color{green}{\\mbox{non-linear}}}$ dependcies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's compare F-test and Mutual Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_regression\n",
    "\n",
    "\n",
    "method_name = 'Mutual Information'\n",
    "feature_scores[method_name] = mutual_info_regression(X_train, y_train)\n",
    "\n",
    "feature_scores[method_name] /= feature_scores[method_name].max()\n",
    "feature_scores.round(2).style.background_gradient(cmap='Reds', high=1.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion**: better estimate for non-linear dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest based feature importances\n",
    "Random forests provide straightforward methods for feature ranking\n",
    "\n",
    "* The importance of a feature is the (normalized) total reduction of the criterion brought by that feature:\n",
    "    * For each split in a tree get how much it decreases Gini index.\n",
    "    * Take sum of this values over all splits of the feature over all trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor \n",
    "\n",
    "method_name = 'Random Forest'\n",
    "random_forest = RandomForestRegressor(n_estimators=100, random_state=random_state)\n",
    "random_forest.fit(X_train, y_train)\n",
    "\n",
    "feature_scores[method_name] = random_forest.feature_importances_.reshape(-1, 1)\n",
    "feature_scores[method_name] /= feature_scores[method_name].max()\n",
    "feature_scores.round(2).style.background_gradient(cmap='Reds', high=1.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L1 and L2 regularizations\n",
    "\n",
    "Regularization adds penalty to a model in order to prevent overfitting and improve generalization.  \n",
    "\n",
    "* Instead of minimizing a loss function $L(X,Y)$, we minimize $L(X,Y)+\\alpha\\|w\\|$,\n",
    "    * where $w$ is the vector of model coefficients,\n",
    "    * $\\|\\cdot\\|$ is typically L1 or L2 norm\n",
    "    * $\\alpha$ is a parameter, specifying the amount of regularization ($\\alpha = 0$ implies no regularization).\n",
    "* **Lasso** - linear model with L1 regularization.\n",
    "* **RidgeRegression** - linear model with L2 regularization\n",
    "* **Lasso** efficiently **zeros out** coefficients, **RidgeRegression** makes them smaller but typically non-zero.\n",
    "* $\\rightarrow$ Lasso can be used for feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso, Ridge\n",
    "\n",
    "method_name = 'Lasso'\n",
    "lasso = Lasso(alpha=0.001, normalize=True).fit(X_train, y_train)\n",
    "feature_scores[method_name] = np.abs(lasso.coef_).reshape(-1, 1)\n",
    "feature_scores[method_name] /= feature_scores[method_name].max()\n",
    "\n",
    "method_name = 'Ridge'\n",
    "ridge = Ridge(normalize=True).fit(X_train, y_train)\n",
    "feature_scores[method_name] = np.abs(ridge.coef_).reshape(-1, 1)\n",
    "feature_scores[method_name] /= feature_scores[method_name].max()\n",
    "\n",
    "feature_scores.round(2).style.background_gradient(cmap='Reds', high=1.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task\n",
    "\n",
    "What if we remove correlated component?\n",
    "\n",
    "**Task**: \n",
    "1. Build `RidgeRegression` model using all features. Check coefficients for features `x4` and `corr(x4)`.\n",
    "2. Build `RidgeRegression` model without the last features. Check coefficient for feature `x4`.\n",
    "3. What can you conclude?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward-backward methods\n",
    "\n",
    "The idea is to add or delete features and look how loss function or some other criteria changes.\n",
    "\n",
    "Decision about adding or deleting a feature may be made based on:\n",
    "\n",
    "- AIC\n",
    "- BIC\n",
    "- validation error\n",
    "- Mallows $C_p$\n",
    "- `model.score()`\n",
    "\n",
    "**Question**: can we use error on the training set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task. Implement forward method\n",
    "\n",
    "Implement the following algorithm:\n",
    "\n",
    "1. Start with empty list of features.\n",
    "2. Add feature that increases model quality the most (use `model.score()` function)\n",
    "3. Iterate step 2 until required number of features is selected\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_steps(X, y, X_valid, y_valid, n_features, model):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ==========\n",
    "    X : np.ndarray\n",
    "        Training inputs\n",
    "\n",
    "    y : np.ndarray\n",
    "        Training targets\n",
    "\n",
    "    X_valid : np.ndarray\n",
    "        Validation inputs\n",
    "\n",
    "    y_valid : np.ndarray\n",
    "        validation targets\n",
    "\n",
    "    n_features: int\n",
    "        Number of features to select\n",
    "\n",
    "    model : sklearn model\n",
    "        \n",
    "    Returns\n",
    "    =======\n",
    "    feature_list : list\n",
    "        List of features\n",
    "    \"\"\"\n",
    "    \n",
    "    feature_list = []\n",
    "    \n",
    "    # Your code here\n",
    "    \n",
    "    # fill variable feat_list with\n",
    "    # the selected features and return it\n",
    "    \n",
    "    return feature_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "features_idx = forward_feat = forward_steps(X_train, y_train,\n",
    "                                            X_valid, y_valid,\n",
    "                                            5, random_forest)\n",
    "print(feature_names[features_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Permutation based feature importance\n",
    "\n",
    "\n",
    "**Idea**: measure how much the score (accuracy, F1, R^2, etc.) decreases when a feature is not available.\n",
    "\n",
    "It can be done by:\n",
    "\n",
    "- [ ] Removing feature from the dataset, re-training the estimator and check the score - can be $\\mathbf{\\color{orange}{\\mbox{computationally intensive}}}$\n",
    "\n",
    "- [x] Replace feature with random noise - now the column doesn't contain useful information.\n",
    "  \n",
    "  * Noise should be from the same distribution as original feature values (as otherwise estimator may fail).\n",
    "  * -> $\\mathbf{\\color{blue}{\\mbox{shuffle values for a feature}}}$, i.e. use other examples’ feature values\n",
    "\n",
    "Some properties:\n",
    "* Allows to estimate feature importance for **generalization** on new data set  \n",
    "* If there are correlated features the importance of both can be low"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task. Implement permutation importance\n",
    "\n",
    "1. Using fitted model calculate its base_score - score, obtained using all features\n",
    "2. Take one feature column and replace its values by the permuted values, calculate\n",
    "   the score and subtract it from base_score, i.e. calculate `score_decrease`\n",
    "3. Repeat step 2 several times and calculte mean `score_decrease`\n",
    "4. Do steps 2 and 3 for each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _permutation_importance(model, X, y, \n",
    "                            feature, n_iters=5, random_state=None):\n",
    "    \"\"\"\n",
    "    Calculate permutation importances for the given feature\n",
    "    \n",
    "    Parameters\n",
    "    ==========\n",
    "    model : sklearn model\n",
    "        Model with `score()` method\n",
    "        \n",
    "    X : np.ndarray\n",
    "        Training inputs\n",
    "        \n",
    "    y : np.ndarray\n",
    "        Training targets\n",
    "        \n",
    "    feature : int\n",
    "        Index of feature importance should be calculated for\n",
    "        \n",
    "    n_iters : int\n",
    "        Number of permutation rounds\n",
    "        \n",
    "    Returns\n",
    "    =======\n",
    "    importance : np.ndarray\n",
    "        Feature importances\n",
    "    \"\"\"\n",
    "    if random_state is None:\n",
    "        random_state = np.random.RandomState(1)\n",
    "    if isinstance(random_state, int):\n",
    "        random_state = np.random.RandomState(random_state)\n",
    "    \n",
    "    importance = None\n",
    "    \n",
    "    # Your code here\n",
    "\n",
    "    return importance\n",
    "\n",
    "def get_permutation_importance(model, X, y, n_iters=100, random_state=None):\n",
    "    permutation_importances = np.zeros(X.shape[1])\n",
    "    for i in range(X.shape[1]):\n",
    "        permutation_importances[i] = _permutation_importance(model, X, y, i)\n",
    "    return permutation_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest = RandomForestRegressor(n_estimators=100).fit(X_train, y_train)\n",
    "permutation_importances = get_permutation_importance(random_forest, X_valid, y_valid)\n",
    "\n",
    "feature_scores['Permutation Importance'] = permutation_importances.reshape(-1, 1)\n",
    "feature_scores['Permutation Importance'] /= feature_scores['Permutation Importance'].max()\n",
    "\n",
    "feature_scores.round(2).style.background_gradient(cmap='Reds', high=1.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is useful to visualize feature importances using bar plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_feature_scores(X, scores):\n",
    "    sorting_idx = scores.argsort()\n",
    "\n",
    "    plt.figure(figsize=(10, 0.3 * len(scores)))\n",
    "    plt.barh(range(len(scores)), scores[sorting_idx], height=0.5)\n",
    "    plt.yticks(range(len(scores)), X.columns[sorting_idx])\n",
    "    for i, v in enumerate(scores[sorting_idx]):\n",
    "\n",
    "        if np.isfinite(v):\n",
    "            v_str = '{:.3f}'.format(v) if v != int(v) else '{}'.format(int(v))\n",
    "            plt.text(v + 0.005 * scores.max(), i - 0.22, v_str)\n",
    "    plt.grid(True)\n",
    "    plt.ylim([-0.5, len(scores) - 0.5])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_feature_scores(pd.DataFrame(X_train), permutation_importances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use RFECV to get optimal number of features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To choose number of features automatically, RFECV module may be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "# use RFECV to select features, use step = 50\n",
    "rfe = RFECV(random_forest, n_jobs=-1, verbose=True, step=1, cv=3)\n",
    "rfe.fit(X_train, y_train)\n",
    "\n",
    "feature_scores['RFECV'] = X.shape[1] - rfe.ranking_.astype(float).reshape(-1, 1)\n",
    "feature_scores['RFECV'] /= feature_scores['RFECV'].max()\n",
    "feature_scores.round(2).style.background_gradient(cmap='Reds', high=1.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output selected number of features\n",
    "print(\"Number selected features =\", np.sum(rfe.support_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task. Calculate quality of the model with the selected features\n",
    "\n",
    "\n",
    "* Use RandomForestRegression\n",
    "* Use test set and model's `score()` function to calculate model error\n",
    "* Try F-test, RandomForest embedded feature importances, Permutation Importance and RFECV\n",
    "* Select 5 features for each feature importance method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_methods(model, X_train, y_train, X_valid, y_valid, X_test, y_test):\n",
    "    methods = {\n",
    "        'F test': lambda X, y, X_valid, y_valid: f_regression(X, y)[0],\n",
    "\n",
    "        'RandomForest': lambda X, y, X_valid, y_valid: model.fit(X, y).feature_importances_,\n",
    "\n",
    "        'PermutationImportance': lambda X, y, X_valid, y_valid: get_permutation_importance(\n",
    "            model.fit(X, y), X_valid, y_valid),\n",
    "\n",
    "        'RFECV': lambda X, y, X_valid, y_valid: -RFECV(\n",
    "            model, n_jobs=-1, step=1, cv=3).fit(X, y).ranking_\n",
    "    }\n",
    "\n",
    "    model_scores = pd.DataFrame(columns=list(methods.keys()))\n",
    "\n",
    "    # Your code here\n",
    "        \n",
    "    return model_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_methods(random_forest, X_train, y_train, X_valid, y_valid, X_test, y_test)\n",
    "\n",
    "results = [compare_methods(random_forest, X_train, y_train,\n",
    "                           X_valid, y_valid, X_test, y_test)\n",
    "           for _ in range(10)]\n",
    "\n",
    "pd.concat(results, axis=0, ignore_index=True).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Akaike Information criterion (AIC)\n",
    "\n",
    "Suppose that we have a statistical model of some data. Let $k$ be the number of estimated parameters in the model.\n",
    "Let $\\hat{L}$ be the maximum value of the likelihood function for the model.\n",
    "Then the AIC value of the model is the following.\n",
    "\n",
    "$$\n",
    "\\mathrm {AIC} = 2k - 2\\ln(\\hat{L})\n",
    "$$\n",
    "\n",
    "\n",
    "* We would like to minimize AIC\n",
    "* AIC rewards goodness of fit (likelihood function)\n",
    "* It includes a penalty - an increasing function of the number of parameters.\n",
    "\n",
    "AIC is founded in information theory. Suppose that the data is generated by some unknown process $f$.\n",
    "We consider two candidate models to represent $f$: $_1$ and $M_2$.\n",
    "If we knew $f$, then we could find the information lost from using $M_1$ to represent $f$ by calculating the Kullback–Leibler divergence,\n",
    "$DKL(f || M_1)$;\n",
    "similarly, the information lost from using $M_2$ to represent $f$ could be found by calculating $DKL(f || M_2)$.\n",
    "We would then choose the candidate model that minimized the information loss.\n",
    "\n",
    "AIC - is an approximate value of how much more information is lost by $M_1$ than by $M_2$.\n",
    "\n",
    "Valid only asymptotically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Information Criterion\n",
    "\n",
    "Defined as \n",
    "\n",
    "$$\n",
    "{\\rm BIC} = \\ln(N)k - 2\\ln(\\hat{L})\n",
    "$$\n",
    "\n",
    "* We would like to minimize BIC\n",
    "* It can be shown that for model $M$ and given data set $D$ the posterior probability is approximately\n",
    "\n",
    "$$\n",
    "p(M | D) \\propto \\exp(-{\\rm BIC}) p(M)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4. Use `LassoLarsIC` to choose optimal alpha based on AIC and BIC\n",
    "\n",
    "* Parameter $\\alpha$ in LASSO affects the number of non-zero coefficients $\\Rightarrow$ allows to control number of parameters in the model\n",
    "* LARS algorithm for lasso for each possible number of non-zero coefficients estimates $\\alpha$ that gives such number of non-zeros.\n",
    "* $\\Rightarrow$ using AIC or BIC we can choose $\\alpha$.\n",
    "\n",
    "**Task**:\n",
    "1. choose $\\alpha$ using AIC, print non-zero feature's names\n",
    "2. choose $\\alpha$ using BIC, print non-zero feature's names\n",
    "3. Compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LassoLarsIC\n",
    "\n",
    "# Your code here\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
