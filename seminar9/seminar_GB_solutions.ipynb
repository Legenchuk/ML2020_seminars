{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seminar: Gradient Boosting\n",
    "Course: Machine Learning by professor Evgeny Burnaev\n",
    "<br>\n",
    "Author: Andrey Lange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Gradient Boosting for regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_1d():\n",
    "    # prepare dataset\n",
    "    n = 1                      # number of features\n",
    "    N = 100**n                 # number of samples\n",
    "    np.random.seed(0)\n",
    "    X = np.random.random((N, n))*3\n",
    "    coeffs = 1 +  2 * np.random.random((n, 1))\n",
    "    y = np.sin(np.matmul(X*X, coeffs)) + np.random.random((N, 1))/3\n",
    "    y = y.ravel()\n",
    "    \n",
    "    return (X, y)\n",
    "    \n",
    "def plot_results(X, y, y_pred, title=''):\n",
    "    plt.plot(X, y, '*b')\n",
    "    plt.plot(X, y_pred, '.r')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('x1')\n",
    "    plt.ylabel('y')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Question 1.\n",
    "In the following example a Gradient Boosting regression is performed by only 1 tree and very small shrinkage `nu=1e-10`. The solution looks like a constant.\n",
    "\n",
    "1.1. Why it looks like a constant?\n",
    "<br>\n",
    "1.2. Try to find that constant value taking into account that `loss='ls'` and draw it on the same plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X, y = get_dataset_1d()\n",
    "\n",
    "clf = GradientBoostingRegressor(loss='ls', max_depth=1, learning_rate=1e-10, n_estimators=1)\n",
    "clf.fit(X, y)\n",
    "y_pred = clf.predict(X)\n",
    "plot_results(X, y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 1.\n",
    "1.1. In case of only 1 tree the final boosting is\n",
    "$$f_1(x) = f_0(x)+\\nu \\sum\\limits_{j=1}^{J} \\gamma_j I(x\\in R_j).$$\n",
    "Since $\\nu$ is too small, $f_1(x)\\approx f_0(x)$ and $f_0(x)$ is the initialization value\n",
    "$$f_0(x) = {\\rm argmin}_{\\gamma} \\sum_{i=1}^{N} L(y, \\gamma).$$\n",
    "\n",
    "1.2.\n",
    "For the loss function $L(y, \\gamma) = (y - \\gamma)^2/2$ it is the mean of targets:\n",
    "$$ f_0(x) = \\frac{1}{N}\\sum_{i=1}^N y_i.$$\n",
    "You can add \n",
    "```\n",
    "plt.plot(X, [np.mean(y)]*len(y), 'ok')\n",
    "```\n",
    "before `plot_results()` call."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Question 2. \n",
    "Solve above question for `loss='lad'`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 2. \n",
    "Change `np.mean()` for `np.median()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Question 3. \n",
    "Some managers of industrial Data Science projects said me that each tree in Gradient Boosting is fit to the targets that are simply the differences $y_i-f_{m-1}(x_i)$ between the values $y_i$ and the current approximation $f_{m-1}(x_i)$ found on the previous step $m-1$. When is it correct?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 3.\n",
    "It is true in case of Gradient Boosting for regression with the squared error loss \n",
    "$$L(y_i, f(x_i)) = \\frac{1}{2}[y_i-f(x_i)]^2.$$\n",
    "\n",
    "\n",
    "The gradients\n",
    "$$ r_{im} = - \\left[\\frac{\\partial L(y_i, f(x_i))}{\\partial f(x_i)}\\right]_{f=f_{m-1}}$$\n",
    "are calculated for each data point $(x_i, y_i)$ on every step $m$. Then a regression tree is fit to the targets $r_{im}$. For the squared error loss the negative gradient is exactly the difference \n",
    "$$r_{im} = y_i-f_{m-1}(x_i).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Question 4*. \n",
    "Actually, the managers from the question above said not only to what the trees are fit but also that the current solution is updated by simply adding each new tree multiplied by a shrinkage parameter and ignore any sophisticated math! :[]\n",
    "\n",
    "The following code shows 2 ways of using the Gradient Boosting for regression:\n",
    "1. Using the class `GradientBoostingRegressor()` - as it should be done if you use scikit-learn\n",
    "2. Our own implementation by adding a tree step by step with shrinkage\n",
    "\n",
    "Ensure that the pictures are the same no matter how you change the hyperparameters `max_depth`, `n_estimators` and `nu`!\n",
    "\n",
    "In our implementation we fit each tree to the targets $y_i-f_{m-1}(x_i)$ (if you answered on the above question you know that it is correct here) multiply by shrinkage $\\nu$ and add to the current model. But it seems that something is missing in our code. No matter how exactly the Gradient Boosting is implemented in scikit-learn, there are many slightly different variants. Assume that it is as described here https://scikit-learn.org/stable/modules/ensemble.html#mathematical-formulation, where the steepest descent chooses the optimal step length\n",
    "$$\n",
    "\\gamma_m = \\arg\\min_{\\gamma} \\sum_{i=1}^{n} \n",
    "L\n",
    "\\left(\n",
    "y_i, f_{m-1}(x_i)\n",
    "- \\gamma \\nabla_F L(y_i, f_{m-1}(x_i))\n",
    "\\right),\n",
    "$$\n",
    "which is used for the model update\n",
    "$$\n",
    "f_m(x) = f_{m-1}(x) - \\gamma_m \\sum_{i=1}^{n} \\nabla_F L(y_i, f_{m-1}(x_i)).\n",
    "$$\n",
    "But we did not implement it! Justify the correctness of our easy implementation of GB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = get_dataset_1d()\n",
    "\n",
    "max_depth = 3\n",
    "n_estimators = 6\n",
    "nu = 0.8                            # shrinkage\n",
    "\n",
    "# usual Gradient Boosting call\n",
    "clf = GradientBoostingRegressor(loss='ls', max_depth=max_depth, learning_rate=nu, n_estimators=n_estimators)\n",
    "clf.fit(X, y)\n",
    "f = clf.predict(X)\n",
    "plot_results(X, y, f, 'Gradient Boosting from scikit-learn')\n",
    "\n",
    "# my Gradient Boosting implementation\n",
    "clf = DecisionTreeRegressor(max_depth=max_depth)\n",
    "f = np.mean(y)                      # initialization\n",
    "for m in range(1, n_estimators+1):\n",
    "    f = f + clf.fit(X, y - f).predict(X) * nu # fit to the difference and shrink\n",
    "plot_results(X, y, f, 'My simple Gradient Boosting, the same as above!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 4*.\n",
    "\n",
    "For the squared error loss $L(y_i, f(x_i)) = \\frac{1}{2}[y_i-f(x_i)]^2$ we have\n",
    "$$ L\\left(y_i, f_{m-1}(x_i) - \\gamma \\nabla_F L(y_i, f_{m-1}(x_i))\\right) =\n",
    " \\frac{1}{2} (1 - \\gamma)^2 (y_i - f_{m-1}(x_i))^2,\n",
    "$$\n",
    "and thus $\\gamma_m=1$ or in other words we can skip the steepest descent in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Gradient Boosting for classification, CatBoost.\n",
    "\n",
    "Lets consider Titanic data! Below is yet another example of easy feature engineering, data preprocessing and the application of Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load data and some easy preprocessing, even Feature Engineering\n",
    "# we use only training dataset because the Kaggle's testing one does not have class labels \n",
    "# and so we can not measure the model quality\n",
    "\n",
    "X_train = pd.read_csv('https://raw.githubusercontent.com/adasegroup/ML2020_seminars/master/seminar9/data/train.csv')\n",
    "y_train = X_train['Survived']\n",
    "X_train = X_train.drop(['PassengerId', 'Survived', 'Name', 'Ticket'], axis=1)\n",
    "\n",
    "# take only the first letter from the Cabin number, which is maybe the ship level...\n",
    "def keep_only_level_of_cabin(X):\n",
    "    idx = X['Cabin'].notnull()\n",
    "    X.loc[idx, 'Cabin'] = [s.strip()[0] for s in X['Cabin'][idx].values]\n",
    "    return X\n",
    "\n",
    "# CatBoost can not process categorical features with NaN, we set them to the string 'MISS'\n",
    "def change_NaN_to_str(X, cat_features_cols):\n",
    "    for col in cat_features_cols:\n",
    "        idx = X[col].isnull()\n",
    "        X.loc[idx, col] = 'MISS'\n",
    "    return X\n",
    "\n",
    "X_train = keep_only_level_of_cabin(X_train)\n",
    "X_train = change_NaN_to_str(X_train, ['Sex', 'Cabin', 'Embarked'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: \n",
    "\n",
    "  1. We use categorical features and CatBoost processes it ok.\n",
    "\n",
    "  \n",
    "  2. Below you can see the tuning of CatBoost with cross-validation, you can play with the hyperparameter ranges.\n",
    "  \n",
    "  \n",
    "  3. You can \"unnatural disable\" cross-validation and ensure that the best score increases (although it is impractical because of overfitting).\n",
    "  \n",
    "  \n",
    "  4. CatBoost processes contineous feature 'Age' despite there are NaNs. It is easy for trees - we can, say, just ignore these values.\n",
    "  \n",
    "  \n",
    "  5. We preprocess the 'Cabin' feature so that it has only the level ('C', 'E', ...). \n",
    "  This is so that there would not be a large number of unique noninformative values. And here we process NaNs as special 'MISS' value because it may mean not the incompletness in the data, but that no any proper value can be for some samples (it is highly correlated with 'Pclass' feature).  \n",
    "  \n",
    "  \n",
    "  6. You can play with feature importances: drop the most important feature and check how the accuracy and the roc_auc reduce. Do the same with the least important feature and feel who affects more. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "clf = CatBoostClassifier(loss_function = 'Logloss',\n",
    "                         # categorical features processing: \n",
    "                         cat_features  = np.where(X_train.columns.isin(['Sex', 'Cabin', 'Embarked']))[0], \n",
    "                         verbose = 0)\n",
    "\n",
    "clf = GridSearchCV(clf,\n",
    "                   # you can play with tuning, up to your CPU performance:\n",
    "                   {'max_depth': [1, 3, 5], 'n_estimators': [50, 150], 'learning_rate': [0.001, 0.01, 0.1, 1]},\n",
    "                   # cross-validation prevents overfitting! you can disable it by setting \n",
    "                   # cv = [(np.array(range(len(y_train))), np.array(range(len(y_train))))]\n",
    "                   cv = 3)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print('best accuracy score:', clf.best_score_) # validation score\n",
    "\n",
    "# feature importances\n",
    "fi = pd.Series(clf.best_estimator_.feature_importances_, index=X_train.columns)\n",
    "fi.sort_values(ascending=False).plot(kind='bar')\n",
    "plt.title('feature importances')\n",
    "plt.show()\n",
    "\n",
    "# now let's draw different ROC curves\n",
    "plt.figure(figsize=[9, 6])\n",
    "fpr, tpr, _ = roc_curve(y_train, clf.predict_proba(X_train)[:, 1])\n",
    "plt.plot(fpr, tpr, 'r', label='train')\n",
    "fpr, tpr, _ = roc_curve(y_train, clf.predict(X_train))\n",
    "plt.plot(fpr, tpr, '--o', label='binary output, train')\n",
    "plt.legend(bbox_to_anchor=(0.999, 1))\n",
    "plt.title('ROC curves')\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
