{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theoretical Mini-seminar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1. Linear Ridge Regression\n",
    "Let us consider the problem of linear ridge regression for data $(x_{1},y_{1}),\\dots,(x_{n},y_{n})\\in\\mathbb{R}^{d\\times 1}$. Let the objects have positive **sample weights** $v_{i}>0$, i.e. the optimization problem is\n",
    "$$\\sum_{i=1}^{n}v_{i}\\cdot L(y_{i}, \\hat{y}_{i})+\\frac{\\lambda}{2}\\|w\\|_{2}^{2}=\\sum_{i=1}^{n}v_{i}\\cdot (\\langle\\boldsymbol{w},\\boldsymbol{x}_{i}\\rangle-y_{i})^{2}+\\frac{\\lambda}{2}\\|w\\|_{2}^{2}\\rightarrow \\min_{\\boldsymbol{w}}.$$\n",
    "This problem reduces to classical linear ridge regression when $v_{i}\\equiv 1$. The matrix form of the problem is\n",
    "$$(Xw-y)^{\\top}V(Xw-y)+\\frac{\\lambda}{2}w^{\\top}w\\rightarrow\\min_{w},$$\n",
    "where $V=V^{\\top}\\in\\mathbb{R}^{n\\times n}$ is the diagonal matrix with diagonal elements $v_{1},\\dots, v_{n}$. Note that the quadratic problem is still convex (w.r.t. $\\boldsymbol{w}$), thus, the solution is unique. Solve this problem for any (symmetric) positive-definite matrix $V$ (not just diagonal) and provide the answer in the matrix form.\n",
    "### Solution:\n",
    "We use matrix derivative formulas (e.g. [this link](http://www.gatsby.ucl.ac.uk/teaching/courses/sntn/sntn-2017/resources/Matrix_derivatives_cribsheet.pdf)) to compute\n",
    "$$\\frac{\\partial }{\\partial w}=2X^{T}V(Xw-y)+\\lambda w=(2wX^{\\top}VX+\\lambda I)w-2X^{\\top}Vy$$\n",
    "We solve $\\frac{\\partial }{\\partial w}=0$ w.r.t. $w$ and obtain the answer:\n",
    "$$w=(X^{\\top}VX+\\frac{\\lambda}{2} I)^{-1}X^{\\top}Vy.$$\n",
    "The matrix $X^{\\top}VX+\\frac{\\lambda}{2} I$ is invertible, since is positive definite. Indeed, $V$ is PD $\\Rightarrow$ $X^{\\top}VX$ is PSD. The sum with PD $\\frac{\\lambda}{2} I$ is PD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2. Logistic Regression\n",
    "Let us consider the case when in the problem of binary classification the training set is lineary separable. Show that in this case the optimization problem for logistic regression (without the bias term) **without L2-regularization** does not have optimum.\n",
    "### Solution\n",
    "For classes $\\{-1, +1\\}$ the problem has the form\n",
    "$$L(\\boldsymbol{w})=\\sum_{i=1}^{n}\\log (1+e^{-y_{i}\\langle \\boldsymbol{x}_{i},\\boldsymbol{w}\\rangle})\\rightarrow \\min_{\\boldsymbol{w}}.$$\n",
    "\n",
    "Let $\\boldsymbol{w}^{*}$ be the hyperplane that separates classes, i.e. $y_{i}\\langle \\boldsymbol{x}_{i},\\boldsymbol{w}^{*}\\rangle>0$ for all $i$. Consider the separating hyperplane $\\lambda \\boldsymbol{w}^{*}$ and note that\n",
    "$$\\lim_{\\lambda\\rightarrow +\\infty}L(\\boldsymbol{\\lambda w^{*}})= \\lim_{\\lambda\\rightarrow +\\infty}\\sum_{i=1}^{n}\\log (1+e^{-\\lambda y_{i}\\langle \\boldsymbol{x}_{i},\\boldsymbol{w}^{*}\\rangle})=0.$$\n",
    "This means that one can make $L(w)$ as close to $0+$ as required. On the other hand, $\\forall\\boldsymbol{w}$ we observe $L(\\boldsymbol{w})>0$. Thus, the infimum of $0$ and it is not achievable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3. Bayesian Naive Classifier\n",
    "Show that in case of $d$ binary-valued features $x_{j}\\in\\{0, 1\\}$ (for $j=1,2,\\dots,d$) Bayesian Naive Classifier's decision rule is linear.\n",
    "### Solution:\n",
    "Bayesian decision rule:\n",
    "$$\\hat{y} = \\underset{y=0,1}{\\operatorname{argmax}} p(y) \\prod_{i=1}^n p(x_i|y)=\\big[\\textbf{apply} \\log\\big]=\\underset{y=0,1}{\\operatorname{argmax}} \\big[\\log p(y) + \\sum_{i=1}^n \\log p(x_i|y)\\big]=$$\n",
    "$$= \\underset{y=0,1}{\\operatorname{argmax}} \\big[\\log p(y) + \\sum_{i=1}^n \\big(x_i \\cdot \\log p(x_i=1|y) + (1 - x_i) \\cdot \\log p(x_i=0|y) \\big)\\big]=$$\n",
    "$$= \\underset{y=0,1}{\\operatorname{argmax}} \\big[\\log p(y) + \\sum_{i=1}^n \\big(x_i \\cdot \\log \\frac{p(x_i=1|y)}{p(x_i=0|y)} + \\log p(x_i=0|y) \\big)\\big]=$$\n",
    "$$= \\underset{y=0,1}{\\operatorname{argmax}} \\big[\\underbrace{\\big(\\log p(y)+\\sum_{i=1}^{n}\\log p(x_i=0|y)\\big)}_{b^{y}} + \n",
    "\\underbrace{\\sum_{i=1}^n x_i\\cdot\\log \\frac{p(x_i=1|y)}{p(x_i=0|y)}}_{w^{y}_{i}}\\big]$$\n",
    "Thus, $p(y=1)>p(y=0)$ is equal to\n",
    "$$\\langle w^{1},x\\rangle+b^{1}>\\langle w^{0},x\\rangle+b^{0}\\Leftrightarrow \\langle w^{1}-w^{0},x\\rangle+b^{1}-b^{0}>0,$$\n",
    "e.g. we have a linear decision rule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4. Simple Kernel\n",
    "Consider a kernel $K:\\mathbb{R}^{2}\\times\\mathbb{R}^{2}\\rightarrow \\mathbb{R}$ defined by $k(x,y)=(x_1y_1x_2^2y_2^2+1)^2 + x_1y_1$. Construct the corresponding feature mapping $\\psi(x)$ such that $k(x,y)=\\langle\\psi(x),\\psi(y)\\rangle$.\n",
    "### Solution:\n",
    "Note that\n",
    "$$k(x,y)=(x_1y_1x_2^2y_2^2+1)^2 + x_1y_1=\\color{red}{x_1^{2}x_2^{4}}\\color{blue}{y_1^{2}y_2^{4}}+2\\color{red}{x_1x_2^2}\\color{blue}{y_1y_2^2}+1+\\color{red}{x_1}\\color{blue}{y_1}=$$\n",
    "$$\\langle\\color{red}{\\underbrace{(x_1^{2}x_{2}^{4},\\sqrt{2}x_{1}x_{2}^{2},1,x_{1})}_{\\psi(x)}},\\color{blue}{\\underbrace{(y_1^{2}y_{2}^{4},\\sqrt{2}y_{1}y_{2}^{2},1,y_{1})}_{\\psi(y)}}\\rangle$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5. Optimal Prediction\n",
    "Assume that the leaf of a **multioutput** regression decision tree contains objects $(x_{i},y_{i})\\in\\mathbb{R}^{d+k}$ for $i=1,2,\\dots,n$. Find the prediction in the leaf which minimizes the mean squared error $\\frac{1}{n}\\sum_{i=1}^{n}L(y_{i},\\hat{y})=\\frac{1}{n}\\sum_{i=1}^{n}\\|y_{i}-\\hat{y}\\|^{2}$ on the train set.\n",
    "### Solution:\n",
    "To minimize the error on the training set we have to find\n",
    "$$\\hat{y}\\in\\underset{y\\in\\mathbb{R}}{\\operatorname{argmin}}\\frac{1}{n}\\sum_{i=1}^{n}\\|y_{i}-\\hat{y}\\|^{2}.$$\n",
    "We compute\n",
    "$$\\frac{1}{n}\\frac{\\partial}{\\partial \\hat{y}}\\sum_{i=1}^{n}\\|y_i-\\hat{y}\\|^{2}=0\\Leftrightarrow \\frac{2}{n}\\sum_{i=1}^{n}(\\hat{y}-y_{i})=0\\Leftrightarrow \\hat{y}=\\frac{\\sum_{i=1}^{n}y_{i}}{n}$$\n",
    "This mean is indeed the minimum (not maximum) and it it is unique. It follows from the fact that quadratic function is (strongly) convex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
